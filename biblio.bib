@article{barredoarrieta_2020,
  title = {Explainable {{Artificial Intelligence}} ({{XAI}}): {{Concepts}}, Taxonomies, Opportunities and Challenges toward Responsible {{AI}}},
  shorttitle = {Explainable {{Artificial Intelligence}} ({{XAI}})},
  author = {Barredo Arrieta, Alejandro and Díaz-Rodríguez, Natalia and Del Ser, Javier and Bennetot, Adrien and Tabik, Siham and Barbado, Alberto and Garcia, Salvador and Gil-Lopez, Sergio and Molina, Daniel and Benjamins, Richard and Chatila, Raja and Herrera, Francisco},
  date = {2020-06-01},
  journaltitle = {Information Fusion},
  shortjournal = {Information Fusion},
  volume = {58},
  pages = {82--115},
  issn = {1566-2535},
  doi = {10.1016/j.inffus.2019.12.012},
  url = {https://www.sciencedirect.com/science/article/pii/S1566253519308103},
  urldate = {2022-01-04},
  abstract = {In the last few years, Artificial Intelligence (AI) has achieved a notable momentum that, if harnessed appropriately, may deliver the best of expectations over many application sectors across the field. For this to occur shortly in Machine Learning, the entire community stands in front of the barrier of explainability, an inherent problem of the latest techniques brought by sub-symbolism (e.g. ensembles or Deep Neural Networks) that were not present in the last hype of AI (namely, expert systems and rule based models). Paradigms underlying this problem fall within the so-called eXplainable AI (XAI) field, which is widely acknowledged as a crucial feature for the practical deployment of AI models. The overview presented in this article examines the existing literature and contributions already done in the field of XAI, including a prospect toward what is yet to be reached. For this purpose we summarize previous efforts made to define explainability in Machine Learning, establishing a novel definition of explainable Machine Learning that covers such prior conceptual propositions with a major focus on the audience for which the explainability is sought. Departing from this definition, we propose and discuss about a taxonomy of recent contributions related to the explainability of different Machine Learning models, including those aimed at explaining Deep Learning methods for which a second dedicated taxonomy is built and examined in detail. This critical literature analysis serves as the motivating background for a series of challenges faced by XAI, such as the interesting crossroads of data fusion and explainability. Our prospects lead toward the concept of Responsible Artificial Intelligence, namely, a methodology for the large-scale implementation of AI methods in real organizations with fairness, model explainability and accountability at its core. Our ultimate goal is to provide newcomers to the field of XAI with a thorough taxonomy that can serve as reference material in order to stimulate future research advances, but also to encourage experts and professionals from other disciplines to embrace the benefits of AI in their activity sectors, without any prior bias for its lack of interpretability.},
  langid = {english},
  keywords = {\_tablet,Accountability,Comprehensibility,Data Fusion,Deep Learning,Explainable Artificial Intelligence,Fairness,Interpretability,Machine Learning,Privacy,Responsible Artificial Intelligence,Transparency},
  file = {C\:\\Users\\MarcoRepetto\\Zotero\\storage\\URU5BHBS\\Barredo Arrieta et al_2020_Explainable Artificial Intelligence (XAI).pdf}
}

@online{ferry_2023,
  title = {Probabilistic {{Dataset Reconstruction}} from {{Interpretable Models}}},
  author = {Ferry, Julien and Aïvodji, Ulrich and Gambs, Sébastien and Huguet, Marie-José and Siala, Mohamed},
  date = {2023-08-29},
  eprint = {2308.15099},
  eprinttype = {arxiv},
  eprintclass = {cs, math},
  url = {http://arxiv.org/abs/2308.15099},
  urldate = {2023-08-30},
  abstract = {Interpretability is often pointed out as a key requirement for trustworthy machine learning. However, learning and releasing models that are inherently interpretable leaks information regarding the underlying training data. As such disclosure may directly conflict with privacy, a precise quantification of the privacy impact of such breach is a fundamental problem. For instance, previous work have shown that the structure of a decision tree can be leveraged to build a probabilistic reconstruction of its training dataset, with the uncertainty of the reconstruction being a relevant metric for the information leak. In this paper, we propose of a novel framework generalizing these probabilistic reconstructions in the sense that it can handle other forms of interpretable models and more generic types of knowledge. In addition, we demonstrate that under realistic assumptions regarding the interpretable models' structure, the uncertainty of the reconstruction can be computed efficiently. Finally, we illustrate the applicability of our approach on both decision trees and rule lists, by comparing the theoretical information leak associated to either exact or heuristic learning algorithms. Our results suggest that optimal interpretable models are often more compact and leak less information regarding their training data than greedily-built ones, for a given accuracy level.},
  pubstate = {preprint},
  version = {1},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Information Theory},
  file = {C\:\\Users\\MarcoRepetto\\Zotero\\storage\\JEK4VV7R\\Ferry et al. - 2023 - Probabilistic Dataset Reconstruction from Interpre.pdf;C\:\\Users\\MarcoRepetto\\Zotero\\storage\\ZAVFQACY\\2308.html}
}

@inproceedings{hancox-li-2020-robus-machin,
  title = {Robustness in Machine Learning Explanations: {{Does}} It Matter?},
  booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
  author = {Hancox-Li, Leif},
  date = {2020},
  series = {{{FAT}}* '20},
  pages = {640--647},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3351095.3372836},
  url = {https://doi.org/10.1145/3351095.3372836},
  abstract = {The explainable AI literature contains multiple notions of what an explanation is and what desiderata explanations should satisfy. One implicit source of disagreement is how far the explanations should reflect real patterns in the data or the world. This disagreement underlies debates about other desiderata, such as how robust explanations are to slight perturbations in the input data. I argue that robustness is desirable to the extent that we're concerned about finding real patterns in the world. The import of real patterns differs according to the problem context. In some contexts, non-robust explanations can constitute a moral hazard. By being clear about the extent to which we care about capturing real patterns, we can also determine whether the Rashomon Effect is a boon or a bane.},
  isbn = {978-1-4503-6936-7},
  pagetotal = {8},
  keywords = {artificial intelligence,epistemology,ethics,explanation,machine learning,methodology,objectivity,philosophy,robustness},
  timestamp = {2023-06-15 11:46:58 (CEST)}
}

@online{hupont_2023,
  title = {Use Case Cards: A Use Case Reporting Framework Inspired by the {{European AI Act}}},
  shorttitle = {Use Case Cards},
  author = {Hupont, Isabelle and Fernández-Llorca, David and Baldassarri, Sandra and Gómez, Emilia},
  date = {2023-06-23},
  eprint = {2306.13701},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2306.13701},
  url = {http://arxiv.org/abs/2306.13701},
  urldate = {2023-07-18},
  abstract = {Despite recent efforts by the Artificial Intelligence (AI) community to move towards standardised procedures for documenting models, methods, systems or datasets, there is currently no methodology focused on use cases aligned with the risk-based approach of the European AI Act (AI Act). In this paper, we propose a new framework for the documentation of use cases, that we call "use case cards", based on the use case modelling included in the Unified Markup Language (UML) standard. Unlike other documentation methodologies, we focus on the intended purpose and operational use of an AI system. It consists of two main parts. Firstly, a UML-based template, tailored to allow implicitly assessing the risk level of the AI system and defining relevant requirements. Secondly, a supporting UML diagram designed to provide information about the system-user interactions and relationships. The proposed framework is the result of a co-design process involving a relevant team of EU policy experts and scientists. We have validated our proposal with 11 experts with different backgrounds and a reasonable knowledge of the AI Act as a prerequisite. We provide the 5 "use case cards" used in the co-design and validation process. "Use case cards" allows framing and contextualising use cases in an effective way, and we hope this methodology can be a useful tool for policy makers and providers for documenting use cases, assessing the risk level, adapting the different requirements and building a catalogue of existing usages of AI.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society},
  file = {C\:\\Users\\MarcoRepetto\\Zotero\\storage\\TBHNXMUU\\Hupont et al. - 2023 - Use case cards a use case reporting framework ins.pdf;C\:\\Users\\MarcoRepetto\\Zotero\\storage\\6WUGUMRK\\2306.html}
}

@article{mitchell-2019-model-cards,
  title = {Model Cards for Model Reporting},
  author = {Mitchell, Margaret and Wu, Simone and Zaldivar, Andrew and Barnes, Parker and Vasserman, Lucy and Hutchinson, Ben and Spitzer, Elena and Raji, Inioluwa Deborah and Gebru, Timnit},
  date = {2019-01},
  journaltitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
  publisher = {{ACM}},
  doi = {10.1145/3287560.3287596},
  url = {http://dx.doi.org/10.1145/3287560.3287596},
  keywords = {replicability},
  timestamp = {2023-05-03 15:57:41 (CEST)}
}

@article{nguyen-2023-fix-fairn,
  title = {Fix Fairness, Don't Ruin Accuracy: {{Performance}} Aware Fairness Repair Using Automl},
  author = {Nguyen, Giang and Biswas, Sumon and Rajan, Hridesh},
  date = {2023},
  journaltitle = {CoRR},
  eprint = {2306.09297v1},
  eprinttype = {arxiv},
  eprintclass = {cs.SE},
  url = {http://arxiv.org/abs/2306.09297v1},
  abstract = {Machine learning (ML) is increasingly being used in critical decision-making software, but incidents have raised questions about the fairness of ML predictions. To address this issue, new tools and methods are needed to mitigate bias in ML-based software. Previous studies have proposed bias mitigation algorithms that only work in specific situations and often result in a loss of accuracy. Our proposed solution is a novel approach that utilizes automated machine learning (AutoML) techniques to mitigate bias. Our approach includes two key innovations: a novel optimization function and a fairness-aware search space. By improving the default optimization function of AutoML and incorporating fairness objectives, we are able to mitigate bias with little to no loss of accuracy. Additionally, we propose a fairness-aware search space pruning method for AutoML to reduce computational cost and repair time. Our approach, built on the state-of-the-art Auto-Sklearn tool, is designed to reduce bias in real-world scenarios. In order to demonstrate the effectiveness of our approach, we evaluated our approach on four fairness problems and 16 different ML models, and our results show a significant improvement over the baseline and existing bias mitigation techniques. Our approach, Fair-AutoML, successfully repaired 60 out of 64 buggy cases, while existing bias mitigation techniques only repaired up to 44 out of 64 cases.}
}

@online{sun_2023,
  title = {Right for the {{Wrong Reason}}: {{Can Interpretable ML Techniques Detect Spurious Correlations}}?},
  shorttitle = {Right for the {{Wrong Reason}}},
  author = {Sun, Susu and Koch, Lisa M. and Baumgartner, Christian F.},
  date = {2023-08-08},
  eprint = {2307.12344},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2307.12344},
  urldate = {2023-08-09},
  abstract = {While deep neural network models offer unmatched classification performance, they are prone to learning spurious correlations in the data. Such dependencies on confounding information can be difficult to detect using performance metrics if the test data comes from the same distribution as the training data. Interpretable ML methods such as post-hoc explanations or inherently interpretable classifiers promise to identify faulty model reasoning. However, there is mixed evidence whether many of these techniques are actually able to do so. In this paper, we propose a rigorous evaluation strategy to assess an explanation technique's ability to correctly identify spurious correlations. Using this strategy, we evaluate five post-hoc explanation techniques and one inherently interpretable method for their ability to detect three types of artificially added confounders in a chest x-ray diagnosis task. We find that the post-hoc technique SHAP, as well as the inherently interpretable Attri-Net provide the best performance and can be used to reliably identify faulty model behavior.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\MarcoRepetto\\Zotero\\storage\\F5PMLP7F\\Sun et al. - 2023 - Right for the Wrong Reason Can Interpretable ML T.pdf;C\:\\Users\\MarcoRepetto\\Zotero\\storage\\9NC7DE2E\\2307.html}
}
