\section{Related works}
Our work can be placed in the recent strand of literature developing frameworks for assessing the risk of AI systems.
Such literature is relatively new and is still in its infancy and started following the work of the High-Level Expert Group on Artificial Intelligence (AI HLEG) \cite{ec2020}.

\cite{benjamins_2021} is one of the first works that proposes a framework for the risk assessment of AI systems.
In the paper a great emphasizes is posed on the need for organizations to make deliberate choices regarding AI and its application to specific problems. What is proposed is a "choices" framework that helps organizations understand and act on the negative consequences of AI before they occur. The framework includes technical choices such as bias, explainability, agency, errors, and continuous learning, as well as generic digital technical choices like privacy, security, and safety. The paper also highlights the significance of transparency and explainability in AI systems.

In \cite{falco_2021} the emphasis is posed on the crucial role of governance and assurance in the application of highly automated systems, particularly in fields that are deemed as critical infrastructure sectors. They propose "AAA" governance principles, namely: prospective Assessments before highly automated systems are implemented; Audit trail to analyze failures and help assess accountability; and system Adherence to jurisdictional requirements for these systems. The authors underline the mandate for transparency and responsibility in handling data related to automated systems, suggesting that data should be shared publicly in an anonymous and secure way to improve system design and foster public trust. They advocate for an independent audit for such systems and propose incentives for organisations to encourage the adoption of these audits.

A different perspective is taken by \cite{vakkuri_2021}. In their paper they examine the application of AI ethics guidelines in practice and discuss the development and iterations of a method called ECCOLA for addressing AI ethics in hands-on applications. Such a framework based on the Essence language of software engineering takes the form of a card-deck format categorised by AI ethics themes. The paper also discusses practical feedback-based improvements made to the ECCOLA cards in terms of layout, readability, and visual appearance.

\cite{winter_2021} instead propose a more systematic approach.
Such an approach is centered around the conception of a comprehensive certification catalogue by TÜV AUSTRIA in collaboration with the Johannes Kepler University Linz and the Software Competence Center Hagenberg.
The foundation of this approach rests on several principles. Firstly, there is the definition of the technical distribution of the application domain, a key element that determines how an AI system should operate within its intended environment. This is followed by a risk-based definition of performance requirements, to ensure the system can meet certain minimum standards even under adverse or unusual conditions. Lastly, there is a statistically valid testing of the final model using independent random samples, providing empirical assurance that the AI system will perform reliably in real-world applications.

The first holistic and flexible framework for the risk assessment of AI systems is proposed in \cite{zicari_2021}.
The paper presents a robust and flexible framework for risk assessment of AI systems known as Z-Inspection. With a focus on applied ethics, it contrasts other toolkits which are predominantly checklists or designed for the early process of design and deployment. Z-Inspection is unique because it can be used for auditing, performing an ethical evaluation over time of already deployed AI systems, and can also be applied by investigators external to the deploying organisation.
The Z-Inspection process involves tasks such as classifying the AI system by domain and usage, reviewing domain-specific frameworks, regulations and laws, developing an evidence base, and listing potential ethical, technical, and legal issues.

\cite{floridi_2022} propose a framework for the ethical assessment of AI systems called capAI. 
CapAI stems from the dire need for ensuring that Artificial Intelligence (AI) systems operate ethically, with the responsibility placed on the organisations that develop and deploy them. It is intended to serve as an additional tool in the management toolbox of organisations to ensure their AI systems strictly adhere to specific ethical principles.
CapAI aims to encourage good software development practices and prevent the most common ethical failures as identified through research. 
Moreover, it supports good governance by following standard methodologies like the Independent Review Panel (IRP), which allows organisations to have a competitive advantage. 
It achieves this by preventing common failures, validating public claims about ethical AI processes, and protecting the organisation’s reputation.
Additionally, capAI is designed to offer practical guidance by converting high-level ethical principles into verifiable criteria to aid in the ethical construction, development, implementation, and utilisation of AI. 
This makes it effective in identifying and rectifying unethical behaviours of AI systems.
Furthermore, capAI gives providers of both ‘high-risk’ and ‘low-risk’ AI systems procedural guidance on verifying claims made about the AI systems they design and deploy. For providers of ‘high-risk’ AI systems, capAI can be used to show compliance with the EU’s Artificial Intelligence Act (AIA). 
For providers of ‘low-risk’ AI systems, they can use capAI to operationalise their commitments to voluntary codes of conduct.
In conclusion, capAI proffers an auditable and standardised process for developing, deploying and operating AI. 
By adopting capAI, the most common failure modes could be prevented, thereby improving the trustworthiness of AI systems.