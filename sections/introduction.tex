\section{Introduction}

Artificial Intelligence (AI) systems are becoming increasingly prevalent in our society, with applications ranging from autonomous vehicles to facial recognition. 
As AI technology continues to advance, it is crucial to ensure that these systems are safe, transparent, and fair.
In an effort to tame this powerful yet uncontrolled area, many countries started developing regulation frameworks.

Among the multitude of such frameworks the one proposed by the European Union (EU) stands out in terms of novelty and maturity.

% L'approccio risk base, magari qui metto uno schemino
One of the relevant feature of the Act is the risk-based approach to AI regulation, by classifying AI systems into four categories according to their potential impact on human rights, safety, and fundamental values:

\begin{itemize}	
    \item Prohibited AI systems are those that are considered unacceptable and contrary to the EUâ€™s values and principles, such as AI systems that manipulate human behavior, opinions, or decisions; AI systems that exploit vulnerabilities of specific groups; AI systems that allow social scoring by governments; etc. These AI systems are banned in the EU.
    
    \item High-risk AI systems are those that are likely to cause significant harm or adverse effects on human rights, safety, or fundamental values, such as AI systems used for biometric identification; AI systems used for critical infrastructure; AI systems used for education or vocational training; AI systems used for employment or workers management; etc. These AI systems are subject to strict obligations and requirements, such as adequate risk assessment and management; high quality and traceability of data and algorithms; transparency and provision of information to users; human oversight and intervention; accuracy, robustness, and security; etc. These AI systems also have to undergo a conformity assessment by a notified body before they can be placed on the market or put into service.
    
    \item Limited-risk AI systems are those that pose some risks to human rights, safety, or fundamental values, but to a lesser extent than high-risk AI systems, such as AI systems used for chatbots; AI systems used for video games; AI systems used for image or video manipulation; etc. These AI systems are subject to transparency obligations, such as informing users that they are interacting with an AI system; disclosing the use of automated image or video manipulation; etc.
    
    \item Minimal-risk AI systems are those that pose no or negligible risks to human rights, safety, or fundamental values, such as AI systems used for spam filters; AI systems used for smart home devices; AI systems used for personal assistants; etc. These AI systems are not subject to any specific obligations or requirements under the regulation, but they have to comply with the general principles and existing laws of the EU.
\end{itemize}

Figure ... 

Given the broad scope of the legislation and the fact that the field is still evolving, a clear example been the inclusion of foundation models after the first draft, the Act does not contain any technical requirements nor means of compliance.

Such a standardization task has been given to the CEN-CENELEC Joint Technical Committee 21.

However as noted by \cite{europeancommission.jointresearchcentre._2023a} the current international standards already provide adequate coverage of the Act.

In particular dimensions such as...

Nonetheless such technical requirements can be difficult to implement by the Decision Maker which is faced by a multitude of conflicting objectives and a budget constraint.

Many authors have proposed qualitative frameworks such as Floridi and ...