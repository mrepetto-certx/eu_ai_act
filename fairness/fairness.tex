\documentclass[12pt]{article}

%\usepackage{mathptmx}       % selects Times Roman as basic font
%\usepackage{helvet}         % selects Helvetica as sans-serif font
%\usepackage{courier}        % selects Courier as typewriter font
%\usepackage{type1cm}        % activate if the above 3 fonts are
%\usepackage{makeidx}         % allows index generation
%\usepackage{graphicx}        % standard LaTeX graphics tool
%\usepackage{multicol}        % used for the two-column index
%\usepackage[bottom]{footmisc}% places footnotes at page bottom
\usepackage[english]{babel}
\usepackage{amsthm}
\newtheorem{definition}{Definition}

\usepackage[backend=biber]{biblatex}
%\addbibresource{../biblio.bib}

%\makeindex             % used for the subject index
                       % please use the style svind.ist with
                       % your makeindex program

\title{Fairness summary}
\date{}

\begin{document}
\maketitle

\section{Definition}

Before introducing the concept and definition of fairness, it is important to dive into the explanation of what algorithmic bias is.

Algorithmic bias refers to consistent and reproducible errors within a computer system (with or without AI engines) that lead to "unfair" results, wherein specific categories are favoured over others in an eventual unaware manner divergent from the algorithm's intended purpose. The main risk is that the outcome can be discriminatory against certain individuals or groups, reinforcing existing social biases. Typically, discrimination refers to \textit{sensitive} variables such as race, gender, and ethnicity. Several examples about this kind of biases can be found in search engine  results, machine translation and image or text generation; see also \cite{algBias} for a literature analysis on this topic.

It is important to underline that the algorithmic bias may not intentional and can emerge from various sources or factors, such as biased training data, algorithms design, or decisions related to model definition. 

As an example, a non-exhaustive list of key factors contributing to algorithmic bias may be:

\begin{itemize}
\item \textbf{Training Data Bias}: If the data used to train an algorithm contains biases, the algorithm may learn and perpetuate those biases in its decision-making processes. For example, if historical data used to train a hiring algorithm reflects existing gender or racial biases, the algorithm may inadvertently perpetuate these biases in its recommendations.

\item \textbf{Algorithmic Design}: The design and structure of an algorithm can also introduce bias. Biased decision-making criteria, flawed assumptions, or inadequate consideration of relevant factors may result in discriminatory outcomes.

\item \textbf{Implementation Bias}: The way an algorithm is implemented or deployed can also contribute to bias. For instance, if an algorithm is applied in a context different from its original design, it may produce unintended and biased results.

\item \textbf{Feedback Loops}: Biased outcomes generated by algorithms can create feedback loops that reinforce existing biases. For example, if a recommendation system suggests certain content to users based on their past preferences, it may unintentionally steer users toward more biased content.
\end{itemize}

Addressing algorithmic bias requires a combination of ethical considerations, careful design, diverse and inclusive development teams, ongoing monitoring, and continuous improvement of algorithms. It is an evolving challenge as technology advances and becomes more ingrained in various aspects of society.

Given the definition of algorithmic bias, fairness is the dimension to monitor in order to mitigate algorithmic bias in processes based on Machine or Deep Learning models. A unique definition of fairness does not exists, and typically research studies try to find a balance between an equality of outcomes or treatment. As an example of discussion, in \cite{defs}, the following definitions of fairness are given and critically analysed. Let $A$ be the set of \textit{sensitive} or (\textit{protected}) attributes, $X$ any other observable attributes and $Y$ the outcome to be predicted:

\begin{definition}
\textbf{(Fairness Through Unawareness (FTU))}: An algorithm is fair so long as any protected attributes $A$ are not explicitly used in the decision-making process.
\end{definition}

\begin{definition}
\textbf{(Individual Fairness (IF))}: An algorithm is fair if it gives similar predictions to similar individuals. Formally, given a metric $d(\cdot, \cdot)$, if individuals $i$ and $j$ are similar under this metric (i.e., $d(i, j)$ is small) then their predictions should be similar: $\hat{Y}(X^{(i)}, A^{(i)}) \approx \hat{Y}(X^{(j)}, A^{(j)})$.
\end{definition}

The main comment on this definitions relates to the complexity of choosing the right metric and the strict requirement of expert domain knowledge to be able to priorly check for relations between $A$ and $X$.


 In the Data Science context, the approach adopted for dealing with fairness is often related to the specific context or task of an application. The main reason is that Machine Learning models during their lifecycle have to monitor not only an algorithmic bias but also a technical one related to models' performances; see \cite{fairea and accuracy} for a discussion on the trade-off between fairness and accuracy of bias mitigation techniques.

\vspace{0.5cm}

INCLUDE REFERENCE PAPER FOR EXAMPLES FROM BEGINNING OF \cite{pipeline}.

\vspace{0.5cm}

\section{Fairness in ML models}

Nowadays, ML models are widely used in different contexts, including several applications in high-stake decision-making such as college admission \cite{college}, recidivism prediction \cite{rec} or hiring processes \cite{hiring}. The development of algorithm is mainly accuracy-oriented, but it is important to consider fairness throughout all the stages in order to assess if its usage may be biased towards sensitive groups. 

Typically, the problem of fairness mitigation is related to classification tasks. 

\subsection{Metrics: group fairness or causal fairness}

Fairness mitigation and verification is divided mainly into two families of metrics:

\begin{itemize}
\item \textbf{Group metrics}: metrics to check if the probability of the positive prediction of the classifier to be approximately equal among different sensitive groups
\item \textbf{Causal metrics}: metrics to assess the direct relation between the outcome and the sensitive features when all the others remains identical
\end{itemize}

Given a dataset $D$ with $n$ instances, let, actual classification label be $Y$ , predicted classification label be $\hat{Y}$ (where $Y = 1$ if the label is favorable to the individuals), and sensitive attribute be $A$ (where $A=1$ is the label for privileged group), the following group metrics can be defined:

\begin{itemize}
\item statistical parity difference (SPD): difference of probability of giving favourable label to each group
\[
SPD = P[ \hat{Y} = 1| A = 0] - P[ \hat{Y} = 1| A = 1]
\]
\item equal opportunity difference (EOD): rue-positive rate difference between groups
\[
EOD = P[ \hat{Y} = 1|Y = 1, A = 0] - P[\hat{Y}  = 1|Y = 1, A = 1]
\]
\item average odds difference (AOD): average of true positive rate and false positive rate differences
\[
AOD = (1/2)\{(P[ \hat{Y} = 1|Y = 1, A = 0] - P[ \hat{Y} = 1|Y = 1, A = 1]) + 
(P[\hat{Y} = 1|Y = 0, A = 0] - P[ \hat{Y}= 1|Y = 0, A = 1])\}
\]
\item error rate difference (ERD): sum of false positive rate difference and false negative rate difference between groups
\[
ERD = \{(P[ \hat{Y} = 1 |Y = 0, A = 0] - P[ \hat{Y}= 1|Y = 0, A = 1])\} + 
\{(P[ \hat{Y}= 0|Y = 1, A = 0] - P[ \hat{Y}= 0 |Y = 1, A = 1])\}
\]
\end{itemize}

\section{Approaches for fairness in ML and algorithms}
%from paper, python library, existing tools

\textit{Ethics by design: an approach to technology ethics and a key component for \textit{responsible innovation} that aims to integrate ethics in the design and development stage of technology. Synonyms  "value-sensitive design" or "ethically aligned design" can be used.}

Bias mitigation approaches can be grouped into main three categories \cite{survey}:

\begin{enumerate}
\item Pre-processing approaches: reduce bias by pre-preprocessing training data, intervening on unbalanced features or removing sensitive ones;
\item In-processing approaches: reduce bias by adapting ML models, using ensemble models or hyperparameters tuning techniques;
\item Post-processing approaches: reduce bias intervening on the predicted outcome, modifying outcome labels.
\end{enumerate}

\subsection{FAIR PREPROCESSING - ML PIPELINE}

\subsection{AUTOML NGUYEN}

\subsection{FAIRNESS VERIFICATION WITH GRAPHICAL MODELS}




\begin{thebibliography}{xx}
 

% \bibitem{abc}
 % \textsc{Author}, \textit{Title}, ...

\bibitem{algBias}
\textsc{Nima Kordzadeh \& Maryam Ghasemaghaei}, \textit{Algorithmic bias: review, synthesis, and future research directions}, European Journal of Information Systems (2021) , DOI: 10.1080/0960085X.2021.1927212

\bibitem{fairea}
\textsc{Max Hort, Jie M Zhang, Federica Sarro \& Mark Harman}, \textit{Fairea: A model behaviour mutation approach to benchmarking bias mitigation methods.}, Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. 994â€“1006 (2021) 

\bibitem{pipeline}
\textsf{Sumon Biswas \& Hridesh Rajan}, \textit{Fair Preprocessing: Towards Understanding Compositional Fairness of Data Transformers in Machine Learning Pipeline}, Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering (2021), DOI: 10.1145/3468264.3468536

\bibitem{college}
Barbara Martinez Neda, Yue Zeng, and Sergio Gago-Masague. Using machine learning in admissions: Reducing human and algorithmic bias in the selection process. In Proceedings of the 52nd ACM Technical Symposium on Computer Science Education, pages 1323â€“1323, 2021.

\bibitem{rec}
Julia Dressel and Hany Farid. The accuracy, fairness, and limits of predicting recidivism. Science advances, 4:eaao5580, 2018.

\bibitem{hiring}
Ifeoma Ajunwa, Sorelle Friedler, Carlos E Scheidegger, and Suresh Venkatasubramanian. Hiring by algorithm: predicting and preventing disparate impact. Available at SSRN, 2016.

\bibitem{accuracy}
Giang Nguyen, Sumon Biswas \& Hridesh Rajan. Fix Fairness, Donâ€™t Ruin Accuracy: Performance Aware Fairness Repair using AutoML. Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (2023). 

\bibitem{survey}
Max Hort, Zhenpeng Chen, Jie M Zhang, Federica Sarro, and Mark Harman. 2022. Bias mitigation for machine learning classifiers: A comprehensive survey. arXiv preprint arXiv:2207.07068 (2022).

\bibitem{defs}
Matt J Kusner, Joshua Loftus, Chris Russell, and Ricardo Silva. Counterfactual fairness. In Advances in neural information processing systems, pages 4066â€“4076, 2017.
\end{thebibliography}


\end{document}
